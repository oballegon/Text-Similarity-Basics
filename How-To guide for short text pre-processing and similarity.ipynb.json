{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text similarity with Python. A simple example for a baseline similarity measure between short texts for social science analysis.\n",
    "\n",
    "In the following notebook, I cover the most basic approaches to pre-processing short texts and vectorizing them in a bag-of-words fashion. This should allow, as we see at the end, to compute similarity metrics between textual data potentially enriching the output of any social-science analysis. \n",
    "\n",
    "This notebook is by no means a complete solution, nor intends to be one, and should just be taken as a minimal \"how-to\". The specificities of each data source, lenght of text and depth of analysis should be taken into account when pre-processing text.\n",
    "\n",
    "Extensive documentation is available from the developpers of each library on each method. Therfore, I don't discuss hyperparameters (when required) beyond the application or what each function does or how. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text.\n",
    "This block imports the main libraries that we're going to use for NLP. Gensim is a very complete toolkit (that actually would allow to do all of the rest, but using NLTK for convenience, since it is widely used in the community.\n",
    "```python\n",
    "\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import PorterStemmer \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line imports to a list the stopwords in English. Being a standard package, this includes the most common cases, \n",
    "```python\n",
    "StopWords = set(stopwords.words('english'))\n",
    "```\n",
    "but we can customize this list, append new characters, or create an entirely new list which we also filter words from:\n",
    "```python\n",
    "customStopWords = [\"omar\",\"is\",\"handsome\"]\n",
    "```   \n",
    "This function Tokenizes Raw Text Input (i.e. a paragraph without any prior pre-processing). \n",
    "The first line of code does (in this order):\n",
    "- Removes dots and replaces them with blankspace.\n",
    "- Splits the text by blankspaces\n",
    "- Removes blankspaces adjacent to words\n",
    "- Sets words to lowercase\n",
    "The second line inside de function transforms the text into a list omitting the previously set stopwords.\n",
    "```python\n",
    "def TokenizeText(RawText):\n",
    "    PreProcessedText = [word.strip().lower() for word in RawText.replace('.',' ').split()]\n",
    "    TokenizedText = [word for word in PreProcessedText if word not in StopWords]\n",
    "    return TokenizedText\n",
    "```\n",
    "We can extend the previous function with additional lines in order to remove, for example, **custom stopwords, special characters, short words** depending on the specificities of our data.\n",
    "```python\n",
    "    #Adding the line below would additionally remove the list of Custom stopwords\n",
    "    TokenizedText = [word for word in PreProcessedText if word not in customStopWords]\n",
    "\n",
    "    #The line below would remove words comprised of one character\n",
    "    TokenizedText = [word for word in TokenizedText if len(word)>1]\n",
    "    \n",
    "    #The following would remove any end-of-word characters in the given list (instead of compressed list, written as a for loop for clarity of code:\n",
    "    TokenizedText2=[]\n",
    "    for token in TokenizedText:\n",
    "        if token[-1] in [\":\",\",\",\"?\",\".\",\";\",\"!\",\")\",\"'\",'\"',\"]\"]:\n",
    "            token=token[:-1]\n",
    "        TokenizedText2.append(token)\n",
    "    TokenizedText = [word for word in TokenizedText2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage example. \n",
    "```python\n",
    "txt1=\"The following text is an example of an abstract. It should allow us to see how the functions work, and especially, develop the next steps in text processing for vectorization.\"\n",
    "txt2=\"Normally text will come from a database, which we organize around an iterable, in this case, a list of texts. This can also be streamed directly from a DB\"\n",
    "data=[txt1,txt2]\n",
    "```\n",
    "Two \"abstracts\" have been loaded into an iterable (a list, in this example). We apply the function \"TokenizeText\" onto the first, to see the output:\n",
    "```python\n",
    "TokenizeText(txt1)\n",
    ">>> ['following',\n",
    " 'text',\n",
    " 'example',\n",
    " 'abstract',\n",
    " 'allow',\n",
    " 'us',\n",
    " 'see',\n",
    " 'functions',\n",
    " 'work',\n",
    " 'especially',\n",
    " 'develop',\n",
    " 'next',\n",
    " 'steps',\n",
    " 'text',\n",
    " 'processing',\n",
    " 'vectorization']\n",
    "```\n",
    "We then need to apply the function sequentially to the iterable. This text is already ready for **TF** or **TF-IDF**, construction. For that we can use either Gensim or Sci-kit Learn libraries.\n",
    "\n",
    "## More pre-processing: Building n-grams of the most common co-occurence of words.\n",
    "There are several approaches that accomplish a similar task. If we have a dictionary of phrases or technical words that belong together, we can pre-load them into NLTK. This step has to be introduced in the TokenizeText function defined above. In the next lines, to se how it works, we run it on txt1. The output will be a list of words where \"next\" and \"steps\" are one token instead of two.\n",
    "```python\n",
    "from nltk.tokenize import MWETokenizer\n",
    "tokenizer = MWETokenizer()\n",
    "tokenizer.add_mwe((\"next\",\"steps\"))\n",
    "tokenizer.tokenize(txt1.split())\n",
    ">>> ['The',\n",
    " 'following',\n",
    " 'text',\n",
    " 'is',\n",
    " 'an',\n",
    " 'example',\n",
    "     ...\n",
    " 'next_steps',\n",
    " 'in',\n",
    " 'text',\n",
    " 'processing',\n",
    " 'for',\n",
    " 'vectorization.']\n",
    "```\n",
    "This isn't always the case, and if we don't know the specificities of our data (or it is very techical) it's best to use different methods learnt from the actual data. It is very useful to benchmark whether $P(\"Next Steps\") > P(\"Next\")*P(\"Steps\")$. There are many pre-trained models about this, but good ones are sometimes complex to use and only capable on \"standard\" text sources. Using simple information metric measures and Gensim, we can perform a similar trick. The example provided below learns normalized pointwise mutual information on the co-ocurrence of words, and joins those that are above the threshold. It takes as an input already-tokenized text.\n",
    "```python\n",
    "phrases=gensim.models.phrases.Phrases([TokenizeText(txt) for txt in data],min_count=1,scoring=\"npmi\",threshold=0.2)\n",
    "bigram=gensim.models.phrases.Phraser(phrases)\n",
    "```\n",
    "We can then apply the learnt \"bigram\" over any list of tokenized text, and the output will include as single tokens the united pairs of words. For higher-order ngrams, we need to iteratively apply the previous method. (In this example, most words will become part of pairs, given that the sample of texts is very limited).\n",
    "```python\n",
    "bigram[TokenizeText(txt1)]\n",
    "\n",
    ">>> ['following_text',\n",
    " 'example_abstract',\n",
    " 'allow_us',\n",
    " 'see_functions',\n",
    " 'work_especially',\n",
    " 'develop_next',\n",
    " 'steps_text',\n",
    " 'processing_vectorization']\n",
    "```\n",
    "\n",
    "## Term-frequency (TF) and TF-iDF (-inverse Document Frequency) and text similarity.\n",
    "First, we need to import a few other libraries in order to make things easier.\n",
    "```python\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "```\n",
    "I'm going to add another \"fake\" abstract to the ones we had, which shares some words with the first, and I'm going to incorporate it into our iterable. This is just to exemplify with a result at the end.\n",
    "```python\n",
    "txt3=\"The following text is another example of an abstract very similar to the first.\"\n",
    "data.append(txt3)\n",
    "```\n",
    "The following lines of code do (in order):\n",
    "- A count of the tokens present in each text from the iterable after tokenization, and multi-word (bigram)-ization.\n",
    "- A transformation into a sparse vector readable for Sci-kit Learn\n",
    "- Storage into variable \"names\" of the meaning of each column in the vector space.\n",
    "\n",
    "- tf-idf and tf trainig on the previous (Last two blocks, one thing each block)\n",
    "\n",
    "```python\n",
    "WordCounts=[Counter(bigram[TokenizeText(txt)]) for txt in data]\n",
    "v= DictVectorizer()\n",
    "ved = v.fit_transform(WordCounts)\n",
    "names = v.get_feature_names()\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True).fit(ved)\n",
    "tfidf = tfidf_transformer.transform(ved)\n",
    "\n",
    "tf_tranformer=TfidfTransformer(use_idf=False,norm=None).fit(ved)\n",
    "tf=tf_tranformer.fit_transform(ved)\n",
    "```\n",
    "\n",
    "The output for either of these Objects (tf / tfidf) is a sparse matrix where each column is a token, and each row is a document. In our case, this should be a matrix of shape 3 (three documents in our data iterable) and **n** tokens\n",
    "```python\n",
    "print(tfidf.shape)\n",
    ">>>(3, 18)\n",
    "```\n",
    "Hence, we can directly apply a metric function in order to measure the similarity amongst these vectors. This results in a diagonal matrix where all the document-vectors are compared to all. The diagonal should normally be 1 (totally similar to itself), and since we introduced a third text that shares words with the first, we should see higher similarity between the first and the third than any other pair:\n",
    "```python\n",
    "cosine_similarity(tf)\n",
    ">>>array([[1.       , 0.       , 0.1767767],\n",
    "       [0.       , 1.       , 0.       ],\n",
    "       [0.1767767, 0.       , 1.       ]])\n",
    "```\n",
    "\n",
    "\n",
    "** Several further characteristics might be incorporated when building tf/tf-idf with TfidfTransformer:\n",
    "- min_df: minimum number of word occurence across all documents\n",
    "- max_df: max number of word occurence across all documents\n",
    "- max_features: only consider top max_features ordered by term frequency across the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
